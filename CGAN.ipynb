{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "latter-nothing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, '2019-06-09 00:10:00+00:00', 0.0, 17.362500000000004, 0.0],\n",
       "       [1, '2019-06-09 00:20:00+00:00', 0.0, 17.2375, 0.0],\n",
       "       [2, '2019-06-09 00:30:00+00:00', 0.0, 17.170588235294115, 0.0],\n",
       "       ...,\n",
       "       [24028, '2019-11-22 23:40:00+00:00', 0.0, 9.08125, 0.0],\n",
       "       [24029, '2019-11-22 23:50:00+00:00', 0.0, 9.08, 0.25],\n",
       "       [24030, '2019-11-23 00:00:00+00:00', 0.0, 9.183333333333334,\n",
       "        1.8458333333333328]], dtype=object)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "N=9\n",
    "nowcast=1\n",
    "\n",
    "y_input_size=1\n",
    "z_input_size=1\n",
    "\n",
    "file=\"univer_segmented.csv\"\n",
    "\n",
    "series=read_csv(file, sep=\",\", header=0,parse_dates=True)\n",
    "series=series.values\n",
    "series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stopped-rogers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "17.362500000000004\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X1=sensors=np.array(series[:,2])\n",
    "X2=sensors=np.array(series[:,3])\n",
    "Y0=sensors=np.array(series[:,4])\n",
    "\n",
    "print(X1[0])\n",
    "print(X2[0])\n",
    "print(Y0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extra-blues",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0 0.0 0.0 ... 0.0 0.0 0.0]\n",
      "(24031,)\n",
      "[0.005448714089054488 0.005409486485821554 0.00538848818056157 ...\n",
      " 0.0028498853748726316 0.002849493098840302 0.0028819212508461936]\n",
      "(24031,)\n",
      "[[0.         0.00544871]\n",
      " [0.         0.00540949]\n",
      " [0.         0.00538849]\n",
      " ...\n",
      " [0.         0.00284989]\n",
      " [0.         0.00284949]\n",
      " [0.         0.00288192]]\n",
      "(24031, 2)\n",
      "(24031, 1)\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " ...\n",
      " [0.        ]\n",
      " [0.25      ]\n",
      " [1.84583333]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "X1n=tf.keras.utils.normalize(    X1, axis=-1, order=2)\n",
    "X1n=np.array(X1n[0])\n",
    "#X1n=X1\n",
    "\n",
    "\n",
    "print(X1n)\n",
    "print(X1n.shape)\n",
    "\n",
    "X2n=tf.keras.utils.normalize(X2, axis=-1, order=2)\n",
    "X2n=np.array(X2n[0])\n",
    "#X2n=X2\n",
    "print(X2n)\n",
    "print(X2n.shape)\n",
    "\n",
    "X0=np.zeros((len(X1n),2))\n",
    "\n",
    "X0[:,0]=X1n\n",
    "X0[:,1]=X2n\n",
    "\n",
    "Y=np.zeros((len(Y0),1))\n",
    "Y[:,0]=Y0\n",
    "\n",
    "print(X0)\n",
    "print(X0.shape)\n",
    "print(Y.shape)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adequate-rebate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24022, 9, 2)\n",
      "(24022, 1)\n",
      "[10456.769375]\n",
      "[0.36726104]\n",
      "max: [28472.30769231]\n",
      "min: [0.]\n"
     ]
    }
   ],
   "source": [
    "X=[]\n",
    "for i in range(N,len(X0)):\n",
    "    X.append(X0[i-N:i])\n",
    "X=np.array(X)    \n",
    "print(X.shape)\n",
    "\n",
    "Y=Y[N-nowcast:len(Y)-nowcast]\n",
    "print(Y.shape)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "print(Y[40])\n",
    "scaler.fit(Y)\n",
    "\n",
    "Y = scaler.transform(Y)\n",
    "print(Y[40])\n",
    "print(\"max:\",scaler.data_max_)\n",
    "print(\"min:\",scaler.data_min_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "immediate-pressure",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.00544871]\n",
      " [0.         0.00540949]\n",
      " [0.         0.00538849]\n",
      " [0.         0.00547189]\n",
      " [0.         0.00538595]\n",
      " [0.         0.00530161]\n",
      " [0.         0.00520943]\n",
      " [0.         0.00517804]\n",
      " [0.         0.00518432]]\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "representative-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train , x_val , y_train , y_val = train_test_split(X , Y ,test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-ownership",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "uniform-semiconductor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 9, 2)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 8, 16)        80          input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 6, 32)        1568        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 4, 64)        6208        conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 4, 64)        0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 4, 32)        12416       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 4, 32)        0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 32)           8320        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          1024        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 32)           0           lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          131328      dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 288)          0           flatten_1[0][0]                  \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          73984       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          32896       dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            129         dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 267,953\n",
      "Trainable params: 267,953\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 9, 2)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model (Functional)              (None, 1)            86577       input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Functional)            (None, 1)            267953      input_1[0][0]                    \n",
      "                                                                 model[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 354,530\n",
      "Trainable params: 86,577\n",
      "Non-trainable params: 267,953\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D,Activation, Dense\n",
    "\n",
    "from tensorflow import Tensor\n",
    "from tensorflow.keras.layers import Input, Conv1D, Conv2D, ReLU, BatchNormalization,\\\n",
    "                                    Add, AveragePooling2D, Flatten, Dense,Dropout,LSTM,TimeDistributed,MaxPooling1D, concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "\n",
    "def make_generator():\n",
    "    x = Input(shape=(N,2))    \n",
    "    x_output = Conv1D(filters=16, kernel_size=2, activation='relu')(x)\n",
    "    x_output = Conv1D(filters=32, kernel_size=3, activation='relu')(x_output)\n",
    "    x_output = Conv1D(filters=64, kernel_size=3, activation='relu')(x_output)\n",
    "    x_output = Dropout(0.25)(x_output)\n",
    "    x_output = LSTM(32, return_sequences=True)(x_output)\n",
    "    x_output = Dropout(0.25)(x_output)\n",
    "    x_output = LSTM(32)(x_output)\n",
    "    latent = Flatten()(x_output)\n",
    "\n",
    "    noise = Input(shape=(z_input_size,))\n",
    "    noise_output = Dense(64, activation='relu')(noise)\n",
    "\n",
    "    flatten = concatenate([latent, noise_output])\n",
    "\n",
    "    \n",
    "    concat = Dense(256)(flatten)\n",
    "    concat = Dense(128)(concat)\n",
    "    y_fake = Dense(1,activation=\"sigmoid\")(concat)\n",
    "\n",
    "    model = Model(inputs=[x,noise], outputs=y_fake)\n",
    "    return model  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_discriminator():\n",
    "    \n",
    "    x = Input(shape=(N,2))    \n",
    "    x_output = Conv1D(filters=16, kernel_size=2, activation='relu')(x)\n",
    "    x_output = Conv1D(filters=32, kernel_size=3, activation='relu')(x_output)\n",
    "    x_output = Conv1D(filters=64, kernel_size=3, activation='relu')(x_output)\n",
    "    x_output = Dropout(0.25)(x_output)\n",
    "    x_output = LSTM(32, return_sequences=True)(x_output)\n",
    "    x_output = Dropout(0.25)(x_output)\n",
    "    x_output = LSTM(32)(x_output)\n",
    "    latent = Flatten()(x_output)\n",
    "\n",
    "    label = Input(shape=(y_input_size,))\n",
    "    label_output = Dense(512, activation='relu')(label)\n",
    "    label_output = Dense(256, activation='relu')(label_output)\n",
    "\n",
    "    concat = concatenate([latent, label_output])\n",
    "    concat = Dense(256)(concat)\n",
    "    concat = Dense(128)(concat)\n",
    "    validity = Dense(1,activation=\"sigmoid\")(concat)\n",
    "\n",
    "    model = Model(inputs=[x, label], outputs=validity)\n",
    "    model.compile(loss=['binary_crossentropy'],optimizer='adam')\n",
    "    model.summary()\n",
    "    return model  \n",
    "\n",
    "x = Input(shape=(N,2))\n",
    "noise = Input(shape=(z_input_size,))\n",
    "generator = make_generator()\n",
    "label_fake = generator([x,noise])\n",
    "\n",
    "\n",
    "discriminator = make_discriminator()\n",
    "\n",
    "validity = discriminator([x, label_fake])\n",
    "\n",
    "discriminator.trainable = False\n",
    "\n",
    "combined = Model([noise, x], [label_fake,validity])\n",
    "combined.compile(loss=['binary_crossentropy'],optimizer='adam',metrics=['accuracy'])\n",
    "#combined.compile(loss=loss_error_gan,optimizer='adam')\n",
    "combined.summary()\n",
    "\n",
    "#combined = Sequential()\n",
    "#combined.add(generator)\n",
    "#combined.add(lier)\n",
    "#combined.add(discriminator)\n",
    "#combined.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "first-principle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19217, 9, 2)\n",
      "(19217, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "#history = generator.fit(x_train,y_train ,\n",
    "#                    batch_size=64,\n",
    "#            validation_data=(x_val,y_val),\n",
    "#            epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dominant-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import confusion_matrix\n",
    "#yhat = generator.predict(x_val)\n",
    "#yhat=scaler.inverse_transform(yhat).flatten()\n",
    "#y_val0=scaler.inverse_transform(y_val).flatten()\n",
    "#print(y_val)\n",
    "#print(yhat)\n",
    "\n",
    "#file = open('prediction.csv', 'w')\n",
    "#file.write('ground truth\\tprediction\\n')\n",
    "#for i in range(0,len(y_val)):\n",
    "#    file.write(str(y_val0[i])+\"\\t\"+str(yhat[i])+\"\\n\")\n",
    "#file.close()\n",
    "\n",
    "#print(\"RMSE\")\n",
    "#error=tf.keras.metrics.mean_squared_error(\n",
    "#    y_val0, yhat\n",
    "#)\n",
    "#print(np.sqrt(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "chief-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_true_samples(batch_size):\n",
    "    idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "    x, true_labels = x_train[idx], y_train[idx]\n",
    "    return x,true_labels\n",
    "\n",
    "def getNoise():\n",
    "    return  np.random.normal(0, 1, (batch_size, z_input_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-harmony",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "modified-yahoo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 0\n",
      "0  of  40\n",
      "2491.064\n",
      "0.087490775\n",
      "pred_true: [[0.9390223 ]\n",
      " [0.47793266]\n",
      " [0.19616109]\n",
      " ...\n",
      " [0.4869357 ]\n",
      " [0.4868406 ]\n",
      " [0.97659457]]\n",
      "[0.9390223  0.97659457 0.97497785 ... 0.97580266 0.97659457 0.97659457]\n",
      "pred_false: [[0.61971945]\n",
      " [0.4865714 ]\n",
      " [0.12902448]\n",
      " ...\n",
      " [0.48645133]\n",
      " [0.47897404]\n",
      " [0.8246533 ]]\n",
      "pred_false: [[0.48778316]\n",
      " [0.47493103]\n",
      " [0.47850442]\n",
      " ...\n",
      " [0.4690289 ]\n",
      " [0.47361508]\n",
      " [0.47745094]]\n",
      "0.087490775\t\n",
      "0.49157127991675337\t0.67516077\n",
      "0.8183142559833507\t0.4092\n",
      "0.9981269510926118\t0.4092\n",
      "e: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-da0222c48938>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetNoise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrue_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_true_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mfake_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uja\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1596\u001b[0m                         '. Consider setting it to AutoShardPolicy.DATA.')\n\u001b[0;32m   1597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1598\u001b[1;33m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1599\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uja\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uja\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uja\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    385\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m     dataset = dataset.map(\n\u001b[0m\u001b[0;32m    388\u001b[0m         grab_batch, num_parallel_calls=dataset_ops.AUTOTUNE)\n\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uja\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[0;32m   1805\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1806\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1807\u001b[1;33m       return ParallelMapDataset(\n\u001b[0m\u001b[0;32m   1808\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1809\u001b[0m           \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uja\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   4240\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4241\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4242\u001b[1;33m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[0;32m   4243\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4244\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uja\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3523\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3524\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3525\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3526\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3527\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uja\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3049\u001b[0m       \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mto\u001b[0m \u001b[0mspecialize\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3050\u001b[0m     \"\"\"\n\u001b[1;32m-> 3051\u001b[1;33m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[0;32m   3052\u001b[0m         *args, **kwargs)\n\u001b[0;32m   3053\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uja\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3017\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3018\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3019\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3020\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3021\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[1;32mc:\\users\\uja\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uja\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3193\u001b[0m     ]\n\u001b[0;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3195\u001b[1;33m     graph_function = ConcreteFunction(\n\u001b[0m\u001b[0;32m   3196\u001b[0m         func_graph_module.func_graph_from_py_func(\n\u001b[0;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uja\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func_graph, attrs, shared_func_graph, function_spec)\u001b[0m\n\u001b[0;32m   1553\u001b[0m     \u001b[1;31m# These each get a reference to the FuncGraph deleter since they use the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1554\u001b[0m     \u001b[1;31m# FuncGraph directly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1555\u001b[1;33m     self._delayed_rewrite_functions = _DelayedRewriteGradientFunctions(\n\u001b[0m\u001b[0;32m   1556\u001b[0m         func_graph, self._attrs, self._garbage_collector)\n\u001b[0;32m   1557\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_order_tape_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uja\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func_graph, attrs, func_graph_deleter)\u001b[0m\n\u001b[0;32m    612\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cached_function_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m     self._inference_function = _EagerDefinedFunction(\n\u001b[0m\u001b[0;32m    615\u001b[0m         \u001b[0m_inference_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m         self._func_graph.inputs, self._func_graph.outputs, attrs)\n",
      "\u001b[1;32mc:\\users\\uja\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, graph, inputs, outputs, attrs)\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m       \u001b[0moutput_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m     fn = pywrap_tf_session.TF_GraphToFunction_wrapper(\n\u001b[0m\u001b[0;32m    448\u001b[0m         \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m         \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "    \n",
    "n_epochs=40\n",
    "batch_size=64\n",
    "\n",
    "valid = np.ones((batch_size, 1))\n",
    "fake = np.zeros((batch_size, 1))\n",
    "\n",
    "\n",
    "file = open('metrics_cgan_class.csv', 'w')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"e:\",epoch)\n",
    "    for batch_idx in range(int(x_train.shape[0] // batch_size)):\n",
    "        \n",
    "        \n",
    "        noise = getNoise()\n",
    "        x,true_labels = generate_true_samples(batch_size)\n",
    "        fake_labels = generator.predict([x,noise])\n",
    "\n",
    "        discriminator.train_on_batch([x, true_labels], valid)\n",
    "        discriminator.train_on_batch([x, fake_labels+0.1* tf.random.uniform(tf.shape(fake_labels))], fake)\n",
    "        \n",
    "        x,true_labels = generate_true_samples(batch_size)\n",
    "        \n",
    "        combined.train_on_batch([noise,x], [true_labels,valid])\n",
    "        \n",
    "        \n",
    "\n",
    "    print(epoch, \" of \", n_epochs)\n",
    "    \n",
    "    \n",
    "    noise = np.random.normal(0, 1, (x_val.shape[0], z_input_size))\n",
    "    yhat = generator.predict([x_val,noise])\n",
    "   \n",
    "    yhat0=scaler.inverse_transform(yhat).flatten()\n",
    "    y_val0=scaler.inverse_transform(y_val).flatten()\n",
    "\n",
    "    error0=tf.keras.metrics.mean_squared_error(\n",
    "        y_val0, yhat0\n",
    "    )\n",
    "    print(np.sqrt(error0))    \n",
    "    file.write(str(np.sqrt(error0))+'\\t')\n",
    "    \n",
    "    error=tf.keras.metrics.mean_squared_error(\n",
    "        y_val.flatten(), yhat.flatten()\n",
    "    )\n",
    "    print(np.sqrt(error))    \n",
    "    file.write(str(np.sqrt(error))+'\\t')\n",
    "    \n",
    "    pred_true=discriminator.predict([x_val, y_val])\n",
    "    print(\"pred_true:\",pred_true)\n",
    "    print(pred_true[pred_true>0.5])\n",
    "    acc_true=len(pred_true[pred_true>=0.5])/len(y_val)   \n",
    "    file.write(str(acc_true)+'\\t')\n",
    "\n",
    "\n",
    "    pred_false=discriminator.predict([x_val, yhat])\n",
    "    print(\"pred_false:\",pred_false)\n",
    "    acc_false=len(pred_false[pred_false<0.5])/len(y_val)   \n",
    "    file.write(str(acc_false)+'\\t')\n",
    "\n",
    "    pred_false2=discriminator.predict([x_val, yhat+0.5* tf.random.uniform(tf.shape(yhat))])\n",
    "    print(\"pred_false:\",pred_false2)\n",
    "    acc_false2=len(pred_false2[pred_false2<0.5])/len(y_val)   \n",
    "    file.write(str(acc_false2)+'\\n')\n",
    "\n",
    "    print(str(np.sqrt(error))+'\\t')\n",
    "\n",
    "    print(str(acc_true)+'\\t'+str(np.mean(pred_true)))\n",
    "    print(str(acc_false)+'\\t'+str(np.mean(pred_false)))\n",
    "    print(str(acc_false2)+'\\t'+str(np.mean(pred_false)))\n",
    "\n",
    "    \n",
    "file.close()        \n",
    "        \n",
    "    #noise = getNoise()\n",
    "    #x,true_labels = generate_true_samples(batch_size)\n",
    "    #fake_labels = generator.predict([x,noise])\n",
    "    #print(\"true_labels:\",true_labels)\n",
    "    #print(\"fake:\",fake_labels)\n",
    "    \n",
    "    #print(\"g_loss:\",g_loss)\n",
    "    #print(\"d_loss:\",d_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "noise = np.random.normal(0, 1, (x_val.shape[0], z_input_size))\n",
    "yhat = generator.predict([x_val,noise])\n",
    "    \n",
    "yhat=scaler.inverse_transform(yhat).flatten()\n",
    "y_val0=scaler.inverse_transform(y_val).flatten()\n",
    "print(y_val)\n",
    "print(yhat)\n",
    "\n",
    "file = open('prediction2.csv', 'w')\n",
    "file.write('ground truth\\tprediction\\n')\n",
    "for i in range(0,len(y_val)):\n",
    "    file.write(str(y_val0[i])+\"\\t\"+str(yhat[i])+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(\"RMSE\")\n",
    "error=tf.keras.metrics.mean_squared_error(\n",
    "    y_val0, yhat\n",
    ")\n",
    "print(np.sqrt(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-rocket",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
